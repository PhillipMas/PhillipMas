{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhillipMas/PhillipMas/blob/main/Parameter_Tuning_Optimization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bbQF-1ScjPpY"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch data\n",
        "ticker = '^J203.JO'  # Example ticker\n",
        "data = yf.download(ticker, start='2012-01-01', end='2024-07-21')\n",
        "data['Log Returns'] = np.log(data['Adj Close'] / data['Adj Close'].shift(1))\n",
        "data = data.dropna()  # Drop NaN values\n",
        "\n",
        "# Prepare data for LSTM\n",
        "window_size = 120\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data['Log Returns'].values.reshape(-1, 1))\n",
        "\n",
        "# Function to create rolling windows\n",
        "def create_rolling_windows(data, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(data)):\n",
        "        X.append(data[i-window_size:i, 0])\n",
        "        y.append(data[i, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create rolling windows\n",
        "X, y = create_rolling_windows(scaled_data, window_size)\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkMpW_NbKZ45",
        "outputId": "345ab816-78ce-4319-a257-bd14d3e258b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2jgxKfvK-dR",
        "outputId": "13faf99e-b359-44f5-8152-99aaae5726f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    units = trial.suggest_int('units', 50, 200, step=50)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.1)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
        "    epochs = trial.suggest_int('epochs', 50, 200, step=50)\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(units=units))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss = model.evaluate(X, y, verbose=0)\n",
        "    return loss\n",
        "\n",
        "# Create a study and optimize the objective function\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(\"Best hyperparameters: \", best_params)\n",
        "\n",
        "# Build and train the model with the best hyperparameters\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=best_params['units'], return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "model.add(Dropout(best_params['dropout_rate']))\n",
        "model.add(LSTM(units=best_params['units']))\n",
        "model.add(Dropout(best_params['dropout_rate']))\n",
        "model.add(Dense(1))\n",
        "\n",
        "optimizer = Adam(learning_rate=best_params['learning_rate'])\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "model.fit(X, y, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS9s1JJ_KZ7a",
        "outputId": "c770aed0-01f7-48ee-f01f-bc940b8c6b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-11 08:21:25,305] A new study created in memory with name: no-name-bcca220f-c6ff-4c0e-960c-0af772945aa4\n",
            "<ipython-input-6-4bd0a84711e7>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "[I 2024-09-11 09:09:56,908] Trial 0 finished with value: 0.11581242084503174 and parameters: {'units': 100, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0017703520437827279, 'batch_size': 96, 'epochs': 150}. Best is trial 0 with value: 0.11581242084503174.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2cGf8NQKZ_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSGNjdG3s1AkMblC2CLsV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}